{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fd5432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.13 (default, Mar 29 2022, 02:18:16) \\n[GCC 7.5.0]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2895a68e",
   "metadata": {},
   "source": [
    "Из демо PostgreSQL базы данных (task_02) загрузите таблицы в PySpark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708c743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/20 11:22:32 WARN Utils: Your hostname, home resolves to a loopback address: 127.0.1.1; using 10.202.32.211 instead (on interface wlp3s0)\n",
      "22/06/20 11:22:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/20 11:22:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/20 11:22:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "dict_keys(['film', 'payment_p2020_06', 'staff'])\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "DB_NAME = 'ded'\n",
    "SELECT_TMP = 'SELECT * FROM {table_name}'\n",
    "\n",
    "\n",
    "appName = \"PySpark PostgreSQL Example - via psycopg2\"\n",
    "master = \"local\"\n",
    "\n",
    "spark = SparkSession.builder.master(master).appName(appName).getOrCreate()\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://postgres:postgres@localhost/{DB_NAME}?client_encoding=utf8\")\n",
    "\n",
    "tables_df = pd.read_sql(\n",
    "    f\"SELECT table_name FROM information_schema.tables WHERE table_type='BASE TABLE' AND table_schema='public';\",\n",
    "    engine)\n",
    "\n",
    "table_name_list = tables_df.table_name\n",
    "\n",
    "# fr_d - frame_dict\n",
    "fr_d = {}\n",
    "error_dict = {}\n",
    "\n",
    "for tname in table_name_list:\n",
    "    query = SELECT_TMP.format(table_name=tname)\n",
    "    try:\n",
    "        frame = spark.createDataFrame(pd.read_sql(query, engine))\n",
    "        fr_d[tname] = frame\n",
    "#         frame.show(1, vertical=True)\n",
    "    except Exception as _e:\n",
    "#         print(f'ERROR with table {tname}')\n",
    "        error_dict[tname] = _e\n",
    "print(error_dict.keys())\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70dc080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, FloatType, MapType, TimestampType, ArrayType\n",
    "\n",
    "film_table = pd.read_sql(\n",
    "    \"SELECT * FROM film;\",\n",
    "    engine)\n",
    "schema = StructType([StructField(\"film_id\", IntegerType(), True),\n",
    "                     StructField(\"title\", StringType(), True),\n",
    "                     StructField(\"description\", StringType(), True),\n",
    "                     StructField(\"release_year\", IntegerType(), True),\n",
    "                     StructField(\"language_id\", IntegerType(), False),\n",
    "                     StructField(\"original_language_id\", IntegerType(), True),\n",
    "                     StructField(\"rental_duration\", IntegerType(), True),\n",
    "                     StructField(\"rental_rate\", FloatType(), True),\n",
    "                     StructField(\"length\", IntegerType(), True),\n",
    "                     StructField(\"replacement_cost\", DoubleType(), True),\n",
    "                     StructField(\"rating\", StringType(), True),\n",
    "                     StructField(\"last_update\", TimestampType(), True),\n",
    "                     StructField(\"special_features\", ArrayType(StringType()), True),\n",
    "                     StructField(\"fulltext\", StringType(), True),])\n",
    "\n",
    "frame = spark.createDataFrame(film_table, schema=schema)\n",
    "fr_d['film'] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee1fc27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           film_actor\n",
       "1              address\n",
       "2                 city\n",
       "3                actor\n",
       "4             category\n",
       "5              country\n",
       "6             customer\n",
       "7              payment\n",
       "8                 film\n",
       "9        film_category\n",
       "10           inventory\n",
       "11            language\n",
       "12    payment_p2020_03\n",
       "13    payment_p2020_02\n",
       "14    payment_p2020_06\n",
       "15              rental\n",
       "16    payment_p2020_05\n",
       "17               staff\n",
       "18               store\n",
       "19    payment_p2020_01\n",
       "20    payment_p2020_04\n",
       "Name: table_name, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b6336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eee393",
   "metadata": {},
   "source": [
    "Вывести количество фильмов в каждой категории, отсортировать по убыванию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9a2377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|category_id|count|\n",
      "+-----------+-----+\n",
      "|         15|   74|\n",
      "|          9|   73|\n",
      "|          8|   69|\n",
      "|          6|   68|\n",
      "|          2|   66|\n",
      "|          1|   64|\n",
      "|         13|   63|\n",
      "|          7|   62|\n",
      "|         10|   61|\n",
      "|         14|   61|\n",
      "|          3|   60|\n",
      "|          5|   58|\n",
      "|          4|   57|\n",
      "|         16|   57|\n",
      "|         11|   56|\n",
      "|         12|   51|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fr_d['film_category'].groupBy(\"category_id\").count().orderBy(-F.col(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0fa62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "film = fr_d['film']\n",
    "film_actor = fr_d['film_actor']\n",
    "actor = fr_d['actor']\n",
    "rental = fr_d['rental']\n",
    "payment = fr_d['payment']\n",
    "film_category = fr_d['film_category']\n",
    "category = fr_d['category']\n",
    "inventory = fr_d['inventory']\n",
    "address = fr_d['address']\n",
    "customer = fr_d['customer']\n",
    "city = fr_d['city']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030dec4e",
   "metadata": {},
   "source": [
    "Вывести 10 актеров, чьи фильмы большего всего арендовали, отсортировать по убыванию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b110f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----+\n",
      "|actor_id|first_name|last_name|summ|\n",
      "+--------+----------+---------+----+\n",
      "|     107|      GINA|DEGENERES| 209|\n",
      "|     102|    WALTER|     TORN| 201|\n",
      "|     198|      MARY|   KEITEL| 192|\n",
      "|     181|   MATTHEW|   CARREY| 190|\n",
      "|     106|   GROUCHO|    DUNST| 183|\n",
      "|      65|    ANGELA|   HUDSON| 183|\n",
      "|      23|    SANDRA|   KILMER| 181|\n",
      "|      60|     HENRY|    BERRY| 180|\n",
      "|      13|       UMA|     WOOD| 179|\n",
      "|     119|    WARREN|  JACKMAN| 178|\n",
      "+--------+----------+---------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "windowSpecAgg = Window.partitionBy(\"rental_duration\").orderBy(\"rental_duration\")\n",
    "\n",
    "film.join(film_actor).where(film.film_id == film_actor.film_id)\\\n",
    "    .join(actor).where(actor.actor_id == film_actor.actor_id)\\\n",
    "    .groupBy(film_actor.actor_id, actor.first_name, actor.last_name)\\\n",
    "    .agg(F.sum(\"rental_duration\").alias(\"summ\"))\\\n",
    "    .orderBy(-F.col(\"summ\")).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e50ef",
   "metadata": {},
   "source": [
    "Вывести категорию фильмов, на которую потратили больше всего денег.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5a7063b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----------------+\n",
      "|category_id|  name|       amount_sum|\n",
      "+-----------+------+-----------------+\n",
      "|         15|Sports|5314.209999999844|\n",
      "+-----------+------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rental\\\n",
    ".join(payment).where(rental.rental_id == payment.rental_id)\\\n",
    ".join(inventory).where(inventory.inventory_id == rental.inventory_id)\\\n",
    ".join(film_category).where(film_category.film_id == inventory.film_id)\\\n",
    ".join(category).where(category.category_id == film_category.category_id)\\\n",
    ".groupBy(category.category_id, category.name)\\\n",
    ".agg(F.sum(\"amount\").alias(\"amount_sum\"))\\\n",
    ".orderBy(-F.col(\"amount_sum\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3bbd7",
   "metadata": {},
   "source": [
    "Вывести названия фильмов, которых нет в inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "660a86d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left_anti = film.join(inventory, film[\"film_id\"] == inventory[\"film_id\"], \"leftanti\").select(F.col(\"title\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efd866",
   "metadata": {},
   "source": [
    "Вывести топ 3 актеров, которые больше всего появлялись в фильмах в категории “Children”. Если у нескольких актеров одинаковое кол-во фильмов, вывести всех.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "606c3df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/20 11:31:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+--------+-----+-----+\n",
      "|actor_id|count|dense|\n",
      "+--------+-----+-----+\n",
      "|      17|    7|    1|\n",
      "|     127|    5|    2|\n",
      "|      80|    5|    2|\n",
      "|      66|    5|    2|\n",
      "|     140|    5|    2|\n",
      "|     187|    4|    3|\n",
      "|      58|    4|    3|\n",
      "|     150|    4|    3|\n",
      "|     142|    4|    3|\n",
      "|     131|    4|    3|\n",
      "|      37|    4|    3|\n",
      "|     101|    4|    3|\n",
      "|      92|    4|    3|\n",
      "|      13|    4|    3|\n",
      "|     173|    4|    3|\n",
      "|      81|    4|    3|\n",
      "|      23|    4|    3|\n",
      "|     109|    4|    3|\n",
      "|      93|    4|    3|\n",
      "+--------+-----+-----+\n",
      "\n",
      "22/06/20 11:33:54 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-2153bc41-9149-4149-977a-6c38e8632d52. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-2153bc41-9149-4149-977a-6c38e8632d52\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:374)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:370)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:370)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:365)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2016)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 54138)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alex/miniconda3/envs/Spark/lib/python3.7/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/alex/miniconda3/envs/Spark/lib/python3.7/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/alex/miniconda3/envs/Spark/lib/python3.7/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/alex/miniconda3/envs/Spark/lib/python3.7/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/alex/miniconda3/envs/Spark/lib/python3.7/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/alex/miniconda3/envs/Spark/lib/python3.7/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/alex/miniconda3/envs/Spark/lib/python3.7/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/alex/miniconda3/envs/Spark/lib/python3.7/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "df1 = film\\\n",
    ".join(film_category).where(film_category.film_id == film.film_id)\\\n",
    ".join(category).where(category.category_id == film_category.category_id)\\\n",
    ".join(film_actor).where(film_actor.film_id == film.film_id)\\\n",
    ".filter(category.name == \"Children\")\\\n",
    ".groupBy(\"actor_id\").agg(F.count(\"actor_id\").alias(\"count\"))\n",
    "\n",
    "\n",
    "window = Window.orderBy(F.col(\"count\").desc())\n",
    "result = df1.withColumn(\"dense\", F.dense_rank().over(window))\n",
    "\n",
    "result.where(F.col(\"dense\") <= 3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce2741a",
   "metadata": {},
   "source": [
    "Вывести города с количеством активных и неактивных клиентов (активный — customer.active = 1).\n",
    "Отсортировать по количеству неактивных клиентов по убыванию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1c90c633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-----+\n",
      "|            city|active|count|\n",
      "+----------------+------+-----+\n",
      "|          Kamyin|     0|    1|\n",
      "|        Xiangfan|     0|    1|\n",
      "|   Coatzacoalcos|     0|    1|\n",
      "|       Najafabad|     0|    1|\n",
      "|         Bat Yam|     0|    1|\n",
      "| Southend-on-Sea|     0|    1|\n",
      "|          Amroha|     0|    1|\n",
      "|        Uluberia|     0|    1|\n",
      "|      Kumbakonam|     0|    1|\n",
      "|          Daxian|     0|    1|\n",
      "|         Wroclaw|     0|    1|\n",
      "|     Szkesfehrvr|     0|    1|\n",
      "|Charlotte Amalie|     0|    1|\n",
      "|          Ktahya|     0|    1|\n",
      "|       Pingxiang|     0|    1|\n",
      "|          London|     1|    2|\n",
      "|          Aurora|     1|    2|\n",
      "|          Moscow|     1|    1|\n",
      "|            Ondo|     1|    1|\n",
      "|         Asuncin|     1|    1|\n",
      "+----------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city\\\n",
    ".join(address).where(address.city_id == city.city_id)\\\n",
    ".join(customer).where(address.address_id == customer.address_id)\\\n",
    ".groupBy(\"city\", \"active\").count().orderBy(F.col(\"active\").asc(), F.col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df1e82",
   "metadata": {},
   "source": [
    "Вывести категорию фильмов, у которой самое большое кол-во часов суммарной аренды в городах (customer.address_id в этом city), и которые начинаются на букву “a”. Тоже самое сделать для городов в которых есть символ “-”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "eaf46d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-cities\n",
      "Sports, 10970 hours\n",
      "Hyphen-cities\n",
      "Foreign, 6472 hours\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "a_cities = city.filter(F.col(\"city\").like(\"A%\"))\n",
    "hyphen_cities = city.filter(F.col(\"city\").like(\"%-%\"))\n",
    "\n",
    "\n",
    "def find_category_with_max_rent_time(cities_to_filter: DataFrame) -> None:\n",
    "    rent_time = cities_to_filter\\\n",
    "        .join(address).where(cities_to_filter.city_id == address.city_id)\\\n",
    "        .join(customer).where(customer.address_id == address.address_id)\\\n",
    "        .join(rental).where(rental.customer_id == customer.customer_id)\\\n",
    "        .withColumn(\"date_diff_sec\",\n",
    "                    F.unix_timestamp(F.col(\"return_date\")) - F.unix_timestamp(F.col(\"rental_date\")))\\\n",
    "            .select(\"rental_id\", \"inventory_id\", \"city\", \"date_diff_sec\")\n",
    "\n",
    "    result_seconds = rent_time\\\n",
    "        .join(inventory).where(rent_time.inventory_id == inventory.inventory_id)\\\n",
    "        .join(film_category).where(inventory.film_id == film_category.film_id)\\\n",
    "        .join(category).where(film_category.category_id == category.category_id)\\\n",
    "        .groupBy(\"name\").sum(\"date_diff_sec\").orderBy(F.col(\"sum(date_diff_sec)\").desc()).first()\n",
    "\n",
    "    print(f'{result_seconds[0]}, {int(result_seconds[1] / 3600)} hours')\n",
    "    \n",
    "\n",
    "print('A-cities')    \n",
    "find_category_with_max_rent_time(a_cities)\n",
    "print('Hyphen-cities')\n",
    "find_category_with_max_rent_time(hyphen_cities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
